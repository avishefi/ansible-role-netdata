# netdata configuration
#
# You can download the latest version of this file, using:
#
#  wget -O /etc/netdata/netdata.conf http://localhost:19999/netdata.conf
# or
#  curl -o /etc/netdata/netdata.conf http://localhost:19999/netdata.conf
#
# You can uncomment and change any of the options below.
# The value shown in the commented settings, is the default value.
#

# global netdata configuration

[global]
	# debug flags = 
	# hostname = 
	history = 3600
	# access log = none
	# OOM score = 1000
	# process scheduling policy = other
	# process nice level = 0
	# glibc malloc arena max for plugins = 1
	# glibc malloc arena max for netdata = 1
	# update every = 1
	# config directory = /etc/netdata
	# log directory = /var/log/netdata
	# plugins directory = /usr/libexec/netdata/plugins.d
	# web files directory = /usr/share/netdata/web
	# cache directory = /var/cache/netdata
	# lib directory = /var/lib/netdata
	# home directory = /var/cache/netdata
	# memory mode = save
	# host access prefix = 
	# memory deduplication (ksm) = yes
	# TZ environment variable = :/etc/localtime
	# debug log = /var/log/netdata/debug.log
	# error log = /var/log/netdata/error.log
	# errors flood protection period = 1200
	# errors to trigger flood protection = 200
	# pthread stack size = 8388608
	# run as user = netdata
	# cleanup obsolete charts after seconds = 3600
	# cleanup orphan hosts after seconds = 3600
	# delete obsolete charts files = yes
	# delete orphan hosts files = yes

[plugins]
	# PATH environment variable = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	# PYTHONPATH environment variable = 
	# tc = yes
	# idlejitter = yes
	# proc = yes
	# diskspace = yes
	# cgroups = yes
	# checks = no
	# enable running new plugins = yes
	# check for new plugins every = 60
	# apps = yes
	# fping = yes
	# python.d = yes
	# charts.d = yes
	# node.d = yes

[registry]
	# enabled = yes
	# registry domain = 
	# registry db directory = /var/lib/netdata/registry
	# netdata unique id file = /var/lib/netdata/registry/netdata.public.unique.id
	# registry db file = /var/lib/netdata/registry/registry.db
	# registry log file = /var/lib/netdata/registry/registry-log.db
	# registry save db every new entries = 1000000
	# registry expire idle persons days = 365
	# registry to announce = https://registry.my-netdata.io
	# registry hostname = london.my-netdata.io
	# verify browser cookies support = yes
	# max URL length = 1024
	# max URL name length = 50

[health]
	# enabled = yes
	# in memory max health log entries = 1000
	# script to execute on alarm = /usr/libexec/netdata/plugins.d/alarm-notify.sh
	# health configuration directory = /etc/netdata/health.d
	# run at least every seconds = 10
	# postpone alarms during hibernation for seconds = 60
	# rotate log every lines = 2000

[backend]
	# host tags = 
	# enabled = no
	# data source = average
	# type = graphite
	# destination = localhost
	# prefix = netdata
	# hostname = london.my-netdata.io
	# update every = 10
	# buffer on failures = 10
	# timeout ms = 20000
	# send names instead of ids = yes
	# send charts matching = *

[web]
	# web files group = netdata
	# web files owner = netdata
	disconnect idle clients after seconds = 3600
	bind to = 127.0.0.1
	# mode = multi-threaded
	# listen backlog = 4096
	# default port = 19999
	# respect do not track policy = no
	# x-frame-options response header = 
	# enable gzip compression = yes
	# gzip compression strategy = default
	# gzip compression level = 3
	# custom dashboard_info.js = 

[statsd]
	# enabled = yes
	# update every (flushInterval) = 1
	# udp messages to process at once = 10
	# create private charts for metrics matching = *
	# max private charts allowed = 200
	# max private charts hard limit = 1000
	# private charts memory mode = save
	# private charts history = 172956
	# decimal detail = 1000
	# histograms and timers percentile (percentThreshold) = 95.00000
	# add dimension for number of events received = yes
	# gaps on gauges (deleteGauges) = no
	# gaps on counters (deleteCounters) = no
	# gaps on meters (deleteMeters) = no
	# gaps on sets (deleteSets) = no
	# gaps on histograms (deleteHistograms) = no
	# gaps on timers (deleteTimers) = no
	# listen backlog = 4096
	# default port = 8125
	# bind to = udp:localhost tcp:localhost


# per plugin configuration

[plugin:apps]
	# update every = 1
	# command options = 

[plugin:charts.d]
	# update every = 1
	# command options = 

[plugin:node.d]
	# update every = 1
	# command options = 

[plugin:proc:/proc/net/dev:lo]
	# enabled = no

[plugin:proc]
	# netdata server resources = yes
	# /proc/stat = yes
	# /proc/uptime = yes
	# /proc/loadavg = yes
	# /proc/sys/kernel/random/entropy_avail = yes
	# /proc/interrupts = yes
	# /proc/softirqs = yes
	# /proc/vmstat = yes
	# /proc/meminfo = yes
	# /sys/kernel/mm/ksm = yes
	# /sys/devices/system/edac/mc = yes
	# /sys/devices/system/node = yes
	# /proc/net/dev = yes
	# /proc/net/netstat = yes
	# /proc/net/snmp = yes
	# /proc/net/snmp6 = yes
	# /proc/net/softnet_stat = yes
	# /proc/net/ip_vs/stats = yes
	# /proc/net/stat/conntrack = yes
	# /proc/net/stat/synproxy = yes
	# /proc/diskstats = yes
	# /proc/net/rpc/nfsd = yes
	# /proc/net/rpc/nfs = yes
	# /proc/spl/kstat/zfs/arcstats = yes
	# ipc = yes

[plugin:idlejitter]
	# loop time in ms = 20

[plugin:tc]
	# script to run to get tc values = /usr/libexec/netdata/plugins.d/tc-qos-helper.sh
	# enable new interfaces detected at runtime = yes
	# enable traffic charts for all interfaces = auto
	# enable packets charts for all interfaces = auto
	# enable dropped charts for all interfaces = auto
	# enable tokens charts for all interfaces = no
	# enable ctokens charts for all interfaces = no
	# enable show all classes and qdiscs for all interfaces = no
	# qos for eth0-ifb = yes
	# traffic chart for eth0-ifb = auto
	# packets chart for eth0-ifb = auto
	# dropped packets chart for eth0-ifb = auto
	# tokens chart for eth0-ifb = no
	# ctokens chart for eth0-ifb = no
	# show all classes for eth0-ifb = no
	# cleanup unused classes every = 120
	# qos for eth0 = yes
	# traffic chart for eth0 = auto
	# packets chart for eth0 = auto
	# dropped packets chart for eth0 = auto
	# tokens chart for eth0 = no
	# ctokens chart for eth0 = no
	# show all classes for eth0 = no

[plugin:proc:/sys/kernel/mm/ksm]
	# /sys/kernel/mm/ksm/pages_shared = /sys/kernel/mm/ksm/pages_shared
	# /sys/kernel/mm/ksm/pages_sharing = /sys/kernel/mm/ksm/pages_sharing
	# /sys/kernel/mm/ksm/pages_unshared = /sys/kernel/mm/ksm/pages_unshared
	# /sys/kernel/mm/ksm/pages_volatile = /sys/kernel/mm/ksm/pages_volatile
	# /sys/kernel/mm/ksm/pages_to_scan = /sys/kernel/mm/ksm/pages_to_scan

[plugin:proc:/proc/loadavg]
	# filename to monitor = /proc/loadavg
	# enable load average = yes
	# enable total processes = yes

[plugin:proc:/proc/interrupts]
	# interrupts per core = yes
	# filename to monitor = /proc/interrupts

[plugin:proc:/proc/softirqs]
	# interrupts per core = yes
	# filename to monitor = /proc/softirqs

[plugin:proc:/proc/sys/kernel/random/entropy_avail]
	# filename to monitor = /proc/sys/kernel/random/entropy_avail

[plugin:proc:/proc/net/dev]
	# enable new interfaces detected at runtime = auto
	# bandwidth for all interfaces = auto
	# packets for all interfaces = auto
	# errors for all interfaces = auto
	# drops for all interfaces = auto
	# fifo for all interfaces = auto
	# compressed packets for all interfaces = auto
	# frames, collisions, carrier counters for all interfaces = auto
	# disable by default interfaces matching = lo fireqos* *-ifb
	# filename to monitor = /proc/net/dev

[plugin:proc:/proc/net/dev:eth0]
	# enabled = yes
	# bandwidth = auto
	# packets = auto
	# errors = auto
	# drops = auto
	# fifo = auto
	# compressed = auto
	# events = auto

[plugin:proc:/proc/net/dev:eth1]
	# enabled = yes
	# bandwidth = auto
	# packets = auto
	# errors = auto
	# drops = auto
	# fifo = auto
	# compressed = auto
	# events = auto

[plugin:proc:/proc/diskstats]
	# enable new disks detected at runtime = yes
	# performance metrics for physical disks = auto
	# performance metrics for virtual disks = auto
	# performance metrics for partitions = no
	# bandwidth for all disks = auto
	# operations for all disks = auto
	# merged operations for all disks = auto
	# i/o time for all disks = auto
	# queued operations for all disks = auto
	# utilization percentage for all disks = auto
	# backlog for all disks = auto
	# remove charts of removed disks = yes
	# path to get block device infos = /sys/dev/block/%lu:%lu/%s
	# path to get h/w sector size = /sys/block/%s/queue/hw_sector_size
	# path to get h/w sector size for partitions = /sys/dev/block/%lu:%lu/subsystem/%s/../queue/hw_sector_size
	# path to device mapper = /dev/mapper
	# filename to monitor = /proc/diskstats
	# exclude disks = loop* ram*
	# performance metrics for disks with major 254 = yes

[plugin:proc:/proc/diskstats:vda]
	queued operations = yes
	# enable = yes
	# enable performance metrics = yes
	# bandwidth = auto
	# operations = auto
	# merged operations = auto
	# i/o time = auto
	# utilization percentage = auto
	# backlog = auto

[plugin:proc:/proc/diskstats:vda1]
	# enable = yes
	# enable performance metrics = no
	# bandwidth = no
	# operations = no
	# merged operations = no
	# i/o time = no
	# queued operations = no
	# utilization percentage = no
	# backlog = no

[plugin:proc:/proc/net/snmp]
	# ipv4 packets = yes
	# ipv4 fragments sent = yes
	# ipv4 fragments assembly = yes
	# ipv4 errors = yes
	# ipv4 TCP connections = yes
	# ipv4 TCP packets = yes
	# ipv4 TCP errors = yes
	# ipv4 TCP handshake issues = yes
	# ipv4 UDP packets = yes
	# ipv4 UDP errors = yes
	# ipv4 ICMP packets = yes
	# ipv4 ICMP messages = yes
	# ipv4 UDPLite packets = yes
	# filename to monitor = /proc/net/snmp

[plugin:proc:/proc/net/snmp6]
	# ipv6 packets = auto
	# ipv6 fragments sent = auto
	# ipv6 fragments assembly = auto
	# ipv6 errors = auto
	# ipv6 UDP packets = auto
	# ipv6 UDP errors = auto
	# ipv6 UDPlite packets = auto
	# ipv6 UDPlite errors = auto
	# bandwidth = auto
	# multicast bandwidth = auto
	# broadcast bandwidth = auto
	# multicast packets = auto
	# icmp = auto
	# icmp redirects = auto
	# icmp errors = auto
	# icmp echos = auto
	# icmp group membership = auto
	# icmp router = auto
	# icmp neighbor = auto
	# icmp mldv2 = auto
	# icmp types = auto
	# ect = auto
	# filename to monitor = /proc/net/snmp6

[plugin:proc:/proc/net/netstat]
	# bandwidth = auto
	# input errors = auto
	# multicast bandwidth = auto
	# broadcast bandwidth = auto
	# multicast packets = auto
	# broadcast packets = auto
	# ECN packets = auto
	# TCP reorders = auto
	# TCP SYN cookies = auto
	# TCP out-of-order queue = auto
	# TCP connection aborts = auto
	# TCP memory pressures = auto
	# filename to monitor = /proc/net/netstat

[plugin:proc:/proc/net/stat/nf_conntrack]
	# filename to monitor = /proc/net/stat/nf_conntrack
	# netfilter new connections = yes
	# netfilter connection changes = yes
	# netfilter connection expectations = yes
	# netfilter connection searches = yes
	# netfilter errors = yes
	# netfilter connections = yes

[plugin:proc:/proc/net/ip_vs_stats]
	# IPVS bandwidth = yes
	# IPVS connections = yes
	# IPVS packets = yes
	# filename to monitor = /proc/net/ip_vs_stats

[plugin:proc:/proc/net/stat/synproxy]
	# SYNPROXY entries = auto
	# SYNPROXY cookies = auto
	# SYNPROXY SYN received = auto
	# SYNPROXY connections reopened = auto
	# filename to monitor = /proc/net/stat/synproxy

[plugin:proc:/proc/stat]
	# cpu utilization = yes
	# per cpu core utilization = yes
	# cpu interrupts = yes
	# context switches = yes
	# processes started = yes
	# processes running = yes
	# filename to monitor = /proc/stat

[plugin:proc:/proc/meminfo]
	# system ram = yes
	# system swap = auto
	# hardware corrupted ECC = auto
	# committed memory = yes
	# writeback memory = yes
	# kernel memory = yes
	# slab memory = yes
	# filename to monitor = /proc/meminfo

[plugin:proc:/proc/vmstat]
	# swap i/o = auto
	# disk i/o = yes
	# memory page faults = yes
	# system-wide numa metric summary = auto
	# filename to monitor = /proc/vmstat

[plugin:proc:/proc/net/rpc/nfsd]
	# filename to monitor = /proc/net/rpc/nfsd
	# read cache = yes
	# file handles = yes
	# I/O = yes
	# threads = yes
	# read ahead = yes
	# network = yes
	# rpc = yes
	# NFS v2 procedures = yes
	# NFS v3 procedures = yes
	# NFS v4 procedures = yes
	# NFS v4 operations = yes

[plugin:cgroups]
	# cgroups plugin resource charts = yes
	# update every = 1
	# check for new cgroups every = 10
	# enable cpuacct stat (total CPU) = auto
	# enable cpuacct usage (per core CPU) = auto
	# enable memory (used mem including cache) = auto
	# enable detailed memory = auto
	# enable memory limits fail count = auto
	# enable swap memory = auto
	# enable blkio bandwidth = auto
	# enable blkio operations = auto
	# enable blkio throttle bandwidth = auto
	# enable blkio throttle operations = auto
	# enable blkio queued operations = auto
	# enable blkio merged operations = auto
	# recheck zero blkio every iterations = 10
	# recheck zero memory failcnt every iterations = 10
	# recheck zero detailed memory every iterations = 10
	# enable systemd services = yes
	# enable systemd services detailed memory = no
	# report used memory without cache = yes
	# path to /sys/fs/cgroup/cpuacct = /sys/fs/cgroup/cpu,cpuacct
	# path to /sys/fs/cgroup/blkio = /sys/fs/cgroup/blkio
	# path to /sys/fs/cgroup/memory = /sys/fs/cgroup/memory
	# path to /sys/fs/cgroup/devices = /sys/fs/cgroup/devices
	# max cgroups to allow = 1000
	# max cgroups depth to monitor = 0
	# enable new cgroups detected at run time = yes
	# enable by default cgroups matching =  !*/init.scope  *.scope  !*/vcpu*  !*/emulator  !*.mount  !*.partition  !*.service  !*.slice  !*.swap  !*.user  !/  !/docker  !/libvirt  !/lxc  !/lxc/*/ns  !/machine  !/qemu  !/system  !/systemd  !/user  * 
	# search for cgroups in subpaths matching =  !*/init.scope  !*-qemu  !/init.scope  !/system  !/systemd  !/user  !/user.slice  !/lxc/*/ns/*  * 
	# script to get cgroup names = /usr/libexec/netdata/plugins.d/cgroup-name.sh
	# script to get cgroup network interfaces = /usr/libexec/netdata/plugins.d/cgroup-network
	# run script to rename cgroups matching =  *.scope  *docker*  *lxc*  *qemu*  !/  !*.mount  !*.partition  !*.service  !*.slice  !*.swap  !*.user  * 
	# cgroups to match as systemd services =  !/system.slice/*/*.service  /system.slice/*.service 
	# enable cgroup / = no
	# search for cgroups under / = yes
	# enable cgroup system.slice = no
	# search for cgroups under /system.slice = yes
	# enable cgroup system.slice/run-rpc_pipefs.mount = no
	# enable cgroup system.slice/-.mount = no
	# enable cgroup system.slice/sys-fs-fuse-connections.mount = no
	# enable cgroup system.slice/proc-sys-fs-binfmt_misc.mount = no
	# enable cgroup system.slice/swap.swap = no
	# enable cgroup system.slice/dev-mqueue.mount = no
	# enable cgroup system.slice/system-getty.slice = no
	# search for cgroups under /system.slice/system-getty.slice = yes
	# enable cgroup system.slice/system-getty.slice/getty_tty1.service = no
	# enable cgroup system.slice/system-postgresql.slice = no
	# search for cgroups under /system.slice/system-postgresql.slice = yes
	# enable cgroup system.slice/system-postgresql.slice/postgresql_9.4-main.service = no
	# enable cgroup system.slice/sys-kernel-debug.mount = no
	# enable cgroup system.slice/dev-hugepages.mount = no
	# enable cgroup user.slice = no

#[plugin:proc:/proc/net/dev:eth0-ifb]
	# enabled = no

[plugin:fping]
	# update every = 1
	# command options = 

[plugin:python.d]
	# update every = 1
	# command options = 

[plugin:proc:diskspace]
	# remove charts of unmounted disks = yes
	# update every = 1
	# check for new mount points every = 15
	# exclude space metrics on paths = /proc/* /sys/* /var/run/user/* /run/user/* /snap/* /var/lib/docker/*
	# exclude space metrics on filesystems = 
	# space usage for all disks = auto
	# inodes usage for all disks = auto

[plugin:proc:diskspace:/dev]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/run]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/dev/shm]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/run/lock]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/sys/fs/cgroup]
	# space usage = no
	# inodes usage = no

[plugin:proc:/sys/devices/system/node]
	# directory to monitor = /sys/devices/system/node
	# enable per-node numa metrics = auto

[plugin:proc:/sys/devices/system/edac/mc]
	# directory to monitor = /sys/devices/system/edac/mc

[plugin:proc:/proc/net/softnet_stat]
	# softnet_stat per core = yes
	# filename to monitor = /proc/net/softnet_stat

[plugin:proc:/proc/sys/net/netfilter/nf_conntrack_max]
	# filename to monitor = /proc/sys/net/netfilter/nf_conntrack_max
	# read every seconds = 10

[plugin:proc:/proc/net/rpc/nfs]
	# filename to monitor = /proc/net/rpc/nfs
	# network = yes
	# rpc = yes
	# NFS v2 procedures = yes
	# NFS v3 procedures = yes
	# NFS v4 procedures = yes

[plugin:proc:diskspace:/sys/kernel/security]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/systemd]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/pstore]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/cpuset]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/cpu,cpuacct]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/memory]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/devices]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/freezer]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/net_cls,net_prio]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/blkio]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/sys/fs/cgroup/perf_event]
	# space usage = no
	# inodes usage = no

[plugin:proc:diskspace:/dev/hugepages]
	# space usage = auto
	# inodes usage = auto

[plugin:proc:diskspace:/proc/sys/fs/binfmt_misc]
	# space usage = no
	# inodes usage = no

#[plugin:proc:/proc/net/dev:vpn0]
	# enabled = yes
	# bandwidth = auto
	# packets = auto
	# errors = auto
	# drops = auto
	# fifo = auto
	# compressed = auto
	# events = auto

[plugin:proc:/proc/spl/kstat/zfs/arcstats]
	# filename to monitor = /proc/spl/kstat/zfs/arcstats
